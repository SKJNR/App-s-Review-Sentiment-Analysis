{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCKH-fLaU7oU",
        "outputId": "2d36f55f-ead4-4961-8598-94bef4d26847"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Table 10: Random Forest with Different VSM Techniques ---\n",
            "|    | VSM Technique   |   Accuracy |\n",
            "|---:|:----------------|-----------:|\n",
            "|  0 | TF/IDF          |     0.8171 |\n",
            "|  1 | TF/IDF(Bigram)  |     0.8166 |\n",
            "|  2 | TF/IDF(Trigram) |     0.8168 |\n",
            "|  3 | TF              |     0.8174 |\n",
            "\n",
            "--- Table 11: Random Forest with and without Emoticons ---\n",
            "|    | VSM Technique            |   Accuracy |\n",
            "|---:|:-------------------------|-----------:|\n",
            "|  0 | TF/IDF with Emoticons    |     0.8171 |\n",
            "|  1 | TF/IDF without Emoticons |     0.8171 |\n",
            "\n",
            "--- Table 12: Gradient Boosting with Different Learning Rates ---\n",
            "|    |   Learning Rate |   Accuracy |\n",
            "|---:|----------------:|-----------:|\n",
            "|  0 |          0.0500 |     0.7536 |\n",
            "|  1 |          0.1000 |     0.7752 |\n",
            "|  2 |          0.2500 |     0.7974 |\n",
            "|  3 |          0.5000 |     0.8077 |\n",
            "|  4 |          0.7500 |     0.8099 |\n",
            "|  5 |          1.0000 |     0.8114 |\n",
            "\n",
            "--- Table 9: Numeric Rating Prediction using Ensemble Classifiers ---\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "\n",
        "# Text cleaning function with option to remove emoticons\n",
        "def clean_text(text, remove_emoticons=True):\n",
        "    if isinstance(text, str):\n",
        "        if remove_emoticons:\n",
        "            text = re.sub(r'[^\\w\\s]', '', text)  # Remove emoticons\n",
        "        text = re.sub(r'[^A-Za-z\\s]', '', text)  # Remove non-alphabetical characters\n",
        "        text = text.lower()\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        words = [word for word in text.split() if word not in stop_words]\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        words = [lemmatizer.lemmatize(word) for word in words]\n",
        "        return ' '.join(words)\n",
        "    else:\n",
        "        return ''\n",
        "\n",
        "# Function to load and preprocess data\n",
        "def load_and_preprocess_data(file_path, remove_emoticons=True):\n",
        "    df = pd.read_csv(file_path)\n",
        "    df = df[['review_description', 'rating']]\n",
        "    df['review_description'] = df['review_description'].astype(str)\n",
        "    df['cleaned_review'] = df['review_description'].apply(lambda x: clean_text(x, remove_emoticons))\n",
        "    df['sentiment'] = df['rating'].apply(label_sentiment)\n",
        "    return df\n",
        "\n",
        "# Function to label sentiment\n",
        "def label_sentiment(rating):\n",
        "    if rating >= 4:\n",
        "        return 'positive'\n",
        "    elif rating == 3:\n",
        "        return 'neutral'\n",
        "    else:\n",
        "        return 'negative'\n",
        "\n",
        "# Function to create additional features\n",
        "def add_text_features(df):\n",
        "    # Adding length of review as a feature\n",
        "    df['review_length'] = df['cleaned_review'].apply(len)\n",
        "\n",
        "    # Adding word count as a feature\n",
        "    df['word_count'] = df['cleaned_review'].apply(lambda x: len(x.split()))\n",
        "\n",
        "    return df\n",
        "\n",
        "# Function to prepare data for modeling\n",
        "def prepare_data_for_modeling(df):\n",
        "    # Vectorize the text data using TF-IDF\n",
        "    vectorizer = TfidfVectorizer(max_features=10000)\n",
        "    X = vectorizer.fit_transform(df['cleaned_review'])\n",
        "\n",
        "    # Encode the sentiment labels\n",
        "    le = LabelEncoder()\n",
        "    y = le.fit_transform(df['sentiment'])\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    return train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest using different VSM techniques\n",
        "def train_rf_with_vsm_techniques(df):\n",
        "    results = []\n",
        "\n",
        "    # TF-IDF Unigram\n",
        "    vectorizer = TfidfVectorizer(max_features=10000)\n",
        "    X = vectorizer.fit_transform(df['cleaned_review'])\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, df['sentiment'], test_size=0.2, random_state=42)\n",
        "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    rf_model.fit(X_train, y_train)\n",
        "    y_pred = rf_model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    results.append({'VSM Technique': 'TF/IDF', 'Accuracy': accuracy})\n",
        "\n",
        "    # TF-IDF Bigrams\n",
        "    vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2))\n",
        "    X = vectorizer.fit_transform(df['cleaned_review'])\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, df['sentiment'], test_size=0.2, random_state=42)\n",
        "    rf_model.fit(X_train, y_train)\n",
        "    y_pred = rf_model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    results.append({'VSM Technique': 'TF/IDF(Bigram)', 'Accuracy': accuracy})\n",
        "\n",
        "    # TF-IDF Trigrams\n",
        "    vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 3))\n",
        "    X = vectorizer.fit_transform(df['cleaned_review'])\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, df['sentiment'], test_size=0.2, random_state=42)\n",
        "    rf_model.fit(X_train, y_train)\n",
        "    y_pred = rf_model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    results.append({'VSM Technique': 'TF/IDF(Trigram)', 'Accuracy': accuracy})\n",
        "\n",
        "    # TF (Term Frequency only)\n",
        "    vectorizer = CountVectorizer(max_features=10000)\n",
        "    X = vectorizer.fit_transform(df['cleaned_review'])\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, df['sentiment'], test_size=0.2, random_state=42)\n",
        "    rf_model.fit(X_train, y_train)\n",
        "    y_pred = rf_model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    results.append({'VSM Technique': 'TF', 'Accuracy': accuracy})\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Function to train Random Forest with and without emoticons\n",
        "def train_rf_with_emoticons(df_with_emoticons, df_without_emoticons):\n",
        "    results = []\n",
        "\n",
        "    # Run Random Forest with Emoticons\n",
        "    vectorizer = TfidfVectorizer(max_features=10000)\n",
        "    X = vectorizer.fit_transform(df_with_emoticons['cleaned_review'])\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, df_with_emoticons['sentiment'], test_size=0.2, random_state=42)\n",
        "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    rf_model.fit(X_train, y_train)\n",
        "    y_pred = rf_model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    results.append({'VSM Technique': 'TF/IDF with Emoticons', 'Accuracy': accuracy})\n",
        "\n",
        "    # Run Random Forest without Emoticons\n",
        "    vectorizer = TfidfVectorizer(max_features=10000)\n",
        "    X = vectorizer.fit_transform(df_without_emoticons['cleaned_review'])\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, df_without_emoticons['sentiment'], test_size=0.2, random_state=42)\n",
        "    rf_model.fit(X_train, y_train)\n",
        "    y_pred = rf_model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    results.append({'VSM Technique': 'TF/IDF without Emoticons', 'Accuracy': accuracy})\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Train Gradient Boosting models with different learning rates\n",
        "def train_gbm_with_different_learning_rates(X_train, X_test, y_train, y_test):\n",
        "    learning_rates = [0.05, 0.1, 0.25, 0.5, 0.75, 1.0]\n",
        "    results = []\n",
        "    for rate in learning_rates:\n",
        "        model = GradientBoostingClassifier(n_estimators=100, learning_rate=rate, random_state=42)\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        results.append({'Learning Rate': rate, 'Accuracy': accuracy})\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Function to train ensemble classifiers and predict numeric ratings\n",
        "def analyze_and_predict(df):\n",
        "    # Vectorize the text\n",
        "    vectorizer = TfidfVectorizer(max_features=10000)\n",
        "    X_text = vectorizer.fit_transform(df['cleaned_review'])\n",
        "\n",
        "    # Prepare numeric rating target\n",
        "    y_numeric = df['rating']\n",
        "\n",
        "    # Prepare sentiment target\n",
        "    le = LabelEncoder()\n",
        "    y_sentiment = le.fit_transform(df['sentiment'])\n",
        "\n",
        "    # Combine text features and additional features\n",
        "    X_features = np.hstack((X_text.toarray(), df[['review_length', 'word_count']].values))\n",
        "\n",
        "    # Split data into train and test sets\n",
        "    X_train, X_test, y_train_numeric, y_test_numeric = train_test_split(X_features, y_numeric, test_size=0.2, random_state=42)\n",
        "    _, _, y_train_sentiment, y_test_sentiment = train_test_split(X_features, y_sentiment, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Models for numeric rating prediction\n",
        "    numeric_models = {\n",
        "        'XGB': XGBClassifier(n_estimators=100, random_state=42),\n",
        "        'RF': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "        'GBM': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
        "        'AB': AdaBoostClassifier(n_estimators=100, random_state=42),\n",
        "        'ET': ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
        "    }\n",
        "\n",
        "    # Model for sentiment analysis (Random Forest)\n",
        "    # Model for sentiment analysis (Random Forest)\n",
        "    sentiment_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "    # Store numeric predictions and discrepancies\n",
        "    numeric_predictions = {}\n",
        "    discrepancies = []\n",
        "\n",
        "    # Train sentiment model\n",
        "    sentiment_model.fit(X_train, y_train_sentiment)\n",
        "    y_pred_sentiment = sentiment_model.predict(X_test)\n",
        "\n",
        "    # Train and predict numeric ratings\n",
        "    for name, model in numeric_models.items():\n",
        "        model.fit(X_train, y_train_numeric)\n",
        "        y_pred_numeric = model.predict(X_test)\n",
        "        numeric_predictions[name] = y_pred_numeric\n",
        "\n",
        "        # Check for discrepancies\n",
        "        for i, (sent_pred, num_pred) in enumerate(zip(y_pred_sentiment, y_pred_numeric)):\n",
        "            if (sent_pred == 2 and num_pred < 3) or (sent_pred == 0 and num_pred >= 4):\n",
        "                discrepancies.append({\n",
        "                    'Model': name,\n",
        "                    'Review': df.iloc[i]['review_description'],\n",
        "                    'Predicted Sentiment': le.inverse_transform([sent_pred])[0],\n",
        "                    'Predicted Rating': num_pred\n",
        "                })\n",
        "\n",
        "    return numeric_predictions, y_test_numeric, y_pred_sentiment, discrepancies\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Main function to handle comprehensive analysis and print all tables\n",
        "def main_full_analysis(file_path):\n",
        "    # Load and preprocess data without and with emoticons for Table 10 & 11\n",
        "    df_with_emoticons = load_and_preprocess_data(file_path, remove_emoticons=False)\n",
        "    df_without_emoticons = load_and_preprocess_data(file_path, remove_emoticons=True)\n",
        "\n",
        "    # Add additional features for analysis (Table 9)\n",
        "    df_with_features = add_text_features(df_with_emoticons)\n",
        "\n",
        "    # Table 10: Random Forest with Different VSM Techniques\n",
        "    print(\"\\n--- Table 10: Random Forest with Different VSM Techniques ---\")\n",
        "    vsm_results = train_rf_with_vsm_techniques(df_without_emoticons)\n",
        "    print(tabulate(vsm_results, headers='keys', tablefmt='pipe', floatfmt='.4f'))\n",
        "\n",
        "    # Table 11: Random Forest with and without Emoticons\n",
        "    print(\"\\n--- Table 11: Random Forest with and without Emoticons ---\")\n",
        "    emoticon_results = train_rf_with_emoticons(df_with_emoticons, df_without_emoticons)\n",
        "    print(tabulate(emoticon_results, headers='keys', tablefmt='pipe', floatfmt='.4f'))\n",
        "\n",
        "    # Table 12: Gradient Boosting with Different Learning Rates\n",
        "    print(\"\\n--- Table 12: Gradient Boosting with Different Learning Rates ---\")\n",
        "    X_train, X_test, y_train, y_test = prepare_data_for_modeling(df_without_emoticons)\n",
        "    gbm_results = train_gbm_with_different_learning_rates(X_train, X_test, y_train, y_test)\n",
        "    print(tabulate(gbm_results, headers='keys', tablefmt='pipe', floatfmt='.4f'))\n",
        "\n",
        "    # Table 9: Numeric Rating Prediction and Aggregation\n",
        "    print(\"\\n--- Table 9: Numeric Rating Prediction using Ensemble Classifiers ---\")\n",
        "    numeric_predictions, y_test_numeric, y_pred_sentiment, discrepancies = analyze_and_predict(df_with_features)\n",
        "\n",
        "    # Aggregate numeric predictions for Table 9\n",
        "    aggregate_results = {'App Name': 'Example App'}  # Placeholder, use actual app names in a loop if processing multiple files\n",
        "    for model_name, preds in numeric_predictions.items():\n",
        "        aggregate_results[model_name] = np.mean(preds)\n",
        "\n",
        "    # Print aggregate results\n",
        "    print(\"\\nAggregate Numeric Predictions (Table 9):\")\n",
        "    for model_name, agg_pred in aggregate_results.items():\n",
        "        print(f\"{model_name}: {agg_pred:.2f}\")\n",
        "\n",
        "    # Print discrepancies identified for validation and insights\n",
        "    print(\"\\nDiscrepancies Identified (Additional Insights):\")\n",
        "    discrepancies_df = pd.DataFrame(discrepancies)\n",
        "    print(tabulate(discrepancies_df, headers='keys', tablefmt='pipe'))\n",
        "\n",
        "    # Save all results to CSV files\n",
        "    vsm_results.to_csv('rf_vsm_results.csv', index=False)\n",
        "    emoticon_results.to_csv('rf_emoticon_results.csv', index=False)\n",
        "    gbm_results.to_csv('gbm_learning_rate_results.csv', index=False)\n",
        "    discrepancies_df.to_csv('discrepancies.csv', index=False)\n",
        "    print(\"\\nAll results have been saved to CSV files.\")\n",
        "\n",
        "# Example usage\n",
        "main_full_analysis('/content/fitbit.csv')\n",
        "\n"
      ]
    }
  ]
}